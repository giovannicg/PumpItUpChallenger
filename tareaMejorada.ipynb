{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.cluster import KMeans\n",
    "from geopy.distance import geodesic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DateFeaturesTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Extrae características de fecha y calcula la edad del punto de agua.\n",
    "    Se encarga de transformar 'date_recorded' y calcular 'waterpoint_age' a partir de 'construction_year'.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.waterpoint_age_median = None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        X = X.copy()\n",
    "        X[\"date_recorded\"] = pd.to_datetime(X[\"date_recorded\"])\n",
    "        X[\"year_recorded\"] = X[\"date_recorded\"].dt.year\n",
    "        X[\"month_recorded\"] = X[\"date_recorded\"].dt.month\n",
    "        X[\"day_recorded\"] = X[\"date_recorded\"].dt.day\n",
    "        X[\"waterpoint_age\"] = X[\"year_recorded\"] - X[\"construction_year\"]\n",
    "        # Calcula la mediana para imputar cuando construction_year==0\n",
    "        self.waterpoint_age_median = X.loc[X[\"construction_year\"] != 0, \"waterpoint_age\"].median()\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        X[\"date_recorded\"] = pd.to_datetime(X[\"date_recorded\"])\n",
    "        X[\"year_recorded\"] = X[\"date_recorded\"].dt.year\n",
    "        X[\"month_recorded\"] = X[\"date_recorded\"].dt.month\n",
    "        X[\"day_recorded\"] = X[\"date_recorded\"].dt.day\n",
    "        X[\"waterpoint_age\"] = X[\"year_recorded\"] - X[\"construction_year\"]\n",
    "        X.loc[X[\"construction_year\"] == 0, \"waterpoint_age\"] = self.waterpoint_age_median\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LocationClusterTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Crea un cluster de ubicación usando las columnas 'longitude' y 'latitude'.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_clusters=10, random_state=42):\n",
    "        self.n_clusters = n_clusters\n",
    "        self.random_state = random_state\n",
    "        self.kmeans = None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.kmeans = KMeans(n_clusters=self.n_clusters, random_state=self.random_state, n_init=10)\n",
    "        self.kmeans.fit(X[[\"longitude\", \"latitude\"]])\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        X[\"location_cluster\"] = self.kmeans.predict(X[[\"longitude\", \"latitude\"]])\n",
    "        return X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegionDistanceTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Calcula la distancia entre la ubicación del punto y el centro (mediana) de la región.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.region_centers = {}\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # Calcula el centro (mediana) de cada región\n",
    "        self.region_centers = X.groupby(\"region\")[[\"latitude\", \"longitude\"]].median().to_dict('index')\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        def calc_distance(row):\n",
    "            region = row[\"region\"]\n",
    "            if region in self.region_centers:\n",
    "                center = self.region_centers[region]\n",
    "                return geodesic((row[\"latitude\"], row[\"longitude\"]),\n",
    "                                (center[\"latitude\"], center[\"longitude\"])).km\n",
    "            else:\n",
    "                return np.nan\n",
    "        X[\"distance_to_region_center\"] = X.apply(calc_distance, axis=1)\n",
    "        return X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carga los datos\n",
    "df_1 = pd.read_csv('data1.csv')\n",
    "df_2 = pd.read_csv('data2.csv')\n",
    "df_target = pd.read_csv('objetivo.csv')\n",
    "\n",
    "# Une los DataFrames de entrenamiento y combina con el target\n",
    "df = pd.concat([df_1, df_2], axis=0)\n",
    "df_final = pd.merge(df, df_target, on=\"id\")\n",
    "\n",
    "# Separa la variable objetivo\n",
    "y = df_final['status_group']\n",
    "X = df_final.drop(columns=['status_group'])\n",
    "\n",
    "# Opcional: Elimina columnas que no vayas a usar\n",
    "columns_to_drop = ['recorded_by']\n",
    "X = X.drop(columns=columns_to_drop)\n",
    "\n",
    "# Define los nombres de las columnas según el procesamiento:\n",
    "# Aquí se asume que las variables numéricas y categóricas son aquellas que usarás en el modelo.\n",
    "# Puedes modificar estas listas según tus necesidades.\n",
    "numeric_features = [\"gps_height\", \"waterpoint_age\", \"longitude\", \"latitude\", \"distance_to_region_center\"]\n",
    "categorical_features = [\"region\", \"location_cluster\", \"public_meeting\", \"permit\"]  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline para el preprocesamiento de datos\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        # Transformación para variables numéricas: imputar y escalar\n",
    "        ('num', Pipeline(steps=[\n",
    "            ('imputer', SimpleImputer(strategy=\"median\")),\n",
    "            ('scaler', MinMaxScaler())\n",
    "        ]), numeric_features),\n",
    "        # Transformación para variables categóricas: imputar y aplicar OneHotEncoding\n",
    "        ('cat', Pipeline(steps=[\n",
    "            ('imputer', SimpleImputer(strategy=\"most_frequent\")),\n",
    "            ('onehot', OneHotEncoder(drop='first', handle_unknown='ignore'))\n",
    "        ]), categorical_features)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline completo que encadena los pasos de feature engineering y el modelo\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('date_features', DateFeaturesTransformer()),\n",
    "    ('location_cluster', LocationClusterTransformer(n_clusters=10)),\n",
    "    ('region_distance', RegionDistanceTransformer()),\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', RandomForestClassifier(n_estimators=100, random_state=42))\n",
    "])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7047138047138047\n",
      "Classification Report:\n",
      "                          precision    recall  f1-score   support\n",
      "\n",
      "             functional       0.74      0.79      0.76      9724\n",
      "functional needs repair       0.42      0.27      0.33      1293\n",
      "         non functional       0.68      0.67      0.68      6803\n",
      "\n",
      "               accuracy                           0.70     17820\n",
      "              macro avg       0.62      0.58      0.59     17820\n",
      "           weighted avg       0.70      0.70      0.70     17820\n",
      "\n",
      "\n",
      "Primeras 5 predicciones:\n",
      "      id    status_group\n",
      "0  50785  non functional\n",
      "1  51630      functional\n",
      "2  17168  non functional\n",
      "3  45559  non functional\n",
      "4  49871  non functional\n"
     ]
    }
   ],
   "source": [
    "# Dividir los datos de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Entrenar el pipeline\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Evaluar el modelo en el set de prueba\n",
    "y_pred = pipeline.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "# ======================\n",
    "# PREDICCIÓN SOBRE NUEVOS DATOS\n",
    "# ======================\n",
    "\n",
    "# Cargar y procesar el DataFrame de predicción\n",
    "prediciton_df = pd.read_csv('predicion.csv')\n",
    "\n",
    "prediciton_df = prediciton_df.drop(columns=columns_to_drop)\n",
    "\n",
    "# Aplica el pipeline entrenado (se aplicarán todos los pasos de transformación de forma consistente)\n",
    "predictions = pipeline.predict(prediciton_df)\n",
    "\n",
    "# Agrega las predicciones y guarda el resultado\n",
    "prediciton_df[\"status_group\"] = predictions\n",
    "final_result = prediciton_df[['id', 'status_group']]\n",
    "final_result.to_csv('resultados_predicciones.csv', index=False)\n",
    "print(\"\\nPrimeras 5 predicciones:\")\n",
    "print(prediciton_df[['id', 'status_group']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columnas en X_temp: ['id', 'amount_tsh', 'date_recorded', 'funder', 'gps_height', 'installer', 'longitude', 'latitude', 'wpt_name', 'num_private', 'basin', 'subvillage', 'region', 'region_code', 'district_code', 'lga', 'ward', 'population', 'public_meeting', 'scheme_management', 'scheme_name', 'permit', 'construction_year', 'extraction_type', 'extraction_type_group', 'extraction_type_class', 'management', 'management_group', 'payment', 'payment_type', 'water_quality', 'quality_group', 'quantity', 'quantity_group', 'source', 'source_type', 'source_class', 'waterpoint_type', 'waterpoint_type_group', 'year_recorded', 'month_recorded', 'day_recorded', 'waterpoint_age', 'location_cluster', 'distance_to_region_center']\n"
     ]
    }
   ],
   "source": [
    "# Aplicar los transformers personalizados manualmente\n",
    "X_temp = X_train.copy()\n",
    "X_temp = pipeline.named_steps['date_features'].transform(X_temp)\n",
    "X_temp = pipeline.named_steps['location_cluster'].transform(X_temp)\n",
    "X_temp = pipeline.named_steps['region_distance'].transform(X_temp)\n",
    "\n",
    "# Imprimir las columnas de X_temp\n",
    "print(\"Columnas en X_temp:\", X_temp.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (41580, 36) número de columnas: 36\n"
     ]
    }
   ],
   "source": [
    "X_train_preprocessed = pipeline.named_steps['preprocessor'].transform(X_temp)\n",
    "feature_names = pipeline.named_steps['preprocessor'].get_feature_names_out()\n",
    "\n",
    "print(\"Shape:\", X_train_preprocessed.shape, \"número de columnas:\", len(feature_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (41580, 36) número de columnas: 36\n"
     ]
    }
   ],
   "source": [
    "X_train_preprocessed = pipeline.named_steps['preprocessor'].transform(X_temp)\n",
    "feature_names = pipeline.named_steps['preprocessor'].get_feature_names_out()\n",
    "\n",
    "print(\"Shape:\", X_train_preprocessed.shape, \"número de columnas:\", len(feature_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Shape of passed values is (41580, 1), indices imply (41580, 36)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Suponiendo que ya tienes X_train_preprocessed (con shape (41580, 36)) y feature_names definidos\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Convertir el array preprocesado a DataFrame\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m X_train_preprocessed_df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_preprocessed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfeature_names\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# 2. Extraer la Importancia de las Características del Modelo\u001b[39;00m\n\u001b[32m      9\u001b[39m importances = pipeline.named_steps[\u001b[33m'\u001b[39m\u001b[33mclassifier\u001b[39m\u001b[33m'\u001b[39m].feature_importances_\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\giova\\Documents\\Python\\PumpItUpChallenger\\.venv\\Lib\\site-packages\\pandas\\core\\frame.py:867\u001b[39m, in \u001b[36mDataFrame.__init__\u001b[39m\u001b[34m(self, data, index, columns, dtype, copy)\u001b[39m\n\u001b[32m    859\u001b[39m         mgr = arrays_to_mgr(\n\u001b[32m    860\u001b[39m             arrays,\n\u001b[32m    861\u001b[39m             columns,\n\u001b[32m   (...)\u001b[39m\u001b[32m    864\u001b[39m             typ=manager,\n\u001b[32m    865\u001b[39m         )\n\u001b[32m    866\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m867\u001b[39m         mgr = \u001b[43mndarray_to_mgr\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    868\u001b[39m \u001b[43m            \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    869\u001b[39m \u001b[43m            \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    870\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    871\u001b[39m \u001b[43m            \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    872\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    873\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtyp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmanager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    876\u001b[39m     mgr = dict_to_mgr(\n\u001b[32m    877\u001b[39m         {},\n\u001b[32m    878\u001b[39m         index,\n\u001b[32m   (...)\u001b[39m\u001b[32m    881\u001b[39m         typ=manager,\n\u001b[32m    882\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\giova\\Documents\\Python\\PumpItUpChallenger\\.venv\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:336\u001b[39m, in \u001b[36mndarray_to_mgr\u001b[39m\u001b[34m(values, index, columns, dtype, copy, typ)\u001b[39m\n\u001b[32m    331\u001b[39m \u001b[38;5;66;03m# _prep_ndarraylike ensures that values.ndim == 2 at this point\u001b[39;00m\n\u001b[32m    332\u001b[39m index, columns = _get_axes(\n\u001b[32m    333\u001b[39m     values.shape[\u001b[32m0\u001b[39m], values.shape[\u001b[32m1\u001b[39m], index=index, columns=columns\n\u001b[32m    334\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m336\u001b[39m \u001b[43m_check_values_indices_shape_match\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    338\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m typ == \u001b[33m\"\u001b[39m\u001b[33marray\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    339\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(values.dtype.type, \u001b[38;5;28mstr\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\giova\\Documents\\Python\\PumpItUpChallenger\\.venv\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:420\u001b[39m, in \u001b[36m_check_values_indices_shape_match\u001b[39m\u001b[34m(values, index, columns)\u001b[39m\n\u001b[32m    418\u001b[39m passed = values.shape\n\u001b[32m    419\u001b[39m implied = (\u001b[38;5;28mlen\u001b[39m(index), \u001b[38;5;28mlen\u001b[39m(columns))\n\u001b[32m--> \u001b[39m\u001b[32m420\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mShape of passed values is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpassed\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, indices imply \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimplied\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mValueError\u001b[39m: Shape of passed values is (41580, 1), indices imply (41580, 36)"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Suponiendo que ya tienes X_train_preprocessed (con shape (41580, 36)) y feature_names definidos\n",
    "# Convertir el array preprocesado a DataFrame\n",
    "X_train_preprocessed_df = pd.DataFrame(X_train_preprocessed, columns=feature_names)\n",
    "# 2. Extraer la Importancia de las Características del Modelo\n",
    "importances = pipeline.named_steps['classifier'].feature_importances_\n",
    "feat_importances = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'importance': importances\n",
    "}).sort_values(by='importance', ascending=False)\n",
    "\n",
    "print(\"Importancia de las 10 características principales:\")\n",
    "print(feat_importances.head(10))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
