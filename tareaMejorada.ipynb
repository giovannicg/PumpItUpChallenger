{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.cluster import KMeans\n",
    "from geopy.distance import geodesic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DateFeaturesTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Extrae características de fecha y calcula la edad del punto de agua.\n",
    "    Se encarga de transformar 'date_recorded' y calcular 'waterpoint_age' a partir de 'construction_year'.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.waterpoint_age_median = None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        X = X.copy()\n",
    "        X[\"date_recorded\"] = pd.to_datetime(X[\"date_recorded\"])\n",
    "        X[\"year_recorded\"] = X[\"date_recorded\"].dt.year\n",
    "        X[\"month_recorded\"] = X[\"date_recorded\"].dt.month\n",
    "        X[\"day_recorded\"] = X[\"date_recorded\"].dt.day\n",
    "        X[\"waterpoint_age\"] = X[\"year_recorded\"] - X[\"construction_year\"]\n",
    "        # Calcula la mediana para imputar cuando construction_year==0\n",
    "        self.waterpoint_age_median = X.loc[X[\"construction_year\"] != 0, \"waterpoint_age\"].median()\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        X[\"date_recorded\"] = pd.to_datetime(X[\"date_recorded\"])\n",
    "        X[\"year_recorded\"] = X[\"date_recorded\"].dt.year\n",
    "        X[\"month_recorded\"] = X[\"date_recorded\"].dt.month\n",
    "        X[\"day_recorded\"] = X[\"date_recorded\"].dt.day\n",
    "        X[\"waterpoint_age\"] = X[\"year_recorded\"] - X[\"construction_year\"]\n",
    "        X.loc[X[\"construction_year\"] == 0, \"waterpoint_age\"] = self.waterpoint_age_median\n",
    "        bins = [-np.inf, 0, 8, 26, 53]\n",
    "        labels = ['Negativo/Inconsistente', 'Muy Baja', 'Baja', 'Alta']\n",
    "        X['waterpoint_age_category'] = pd.cut(X['waterpoint_age'], bins=bins, labels=labels, include_lowest=True)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LocationClusterTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Crea un cluster de ubicación usando las columnas 'longitude' y 'latitude'.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_clusters=10, random_state=42):\n",
    "        self.n_clusters = n_clusters\n",
    "        self.random_state = random_state\n",
    "        self.kmeans = None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.kmeans = KMeans(n_clusters=self.n_clusters, random_state=self.random_state, n_init=10)\n",
    "        self.kmeans.fit(X[[\"longitude\", \"latitude\"]])\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        X[\"location_cluster\"] = self.kmeans.predict(X[[\"longitude\", \"latitude\"]])\n",
    "        return X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegionDistanceTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Calcula la distancia entre la ubicación del punto y el centro (mediana) de la región.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.region_centers = {}\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # Calcula el centro (mediana) de cada región\n",
    "        self.region_centers = X.groupby(\"region\")[[\"latitude\", \"longitude\"]].median().to_dict('index')\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        def calc_distance(row):\n",
    "            region = row[\"region\"]\n",
    "            if region in self.region_centers:\n",
    "                center = self.region_centers[region]\n",
    "                return geodesic((row[\"latitude\"], row[\"longitude\"]),\n",
    "                                (center[\"latitude\"], center[\"longitude\"])).km\n",
    "            else:\n",
    "                return np.nan\n",
    "        X[\"distance_to_region_center\"] = X.apply(calc_distance, axis=1)\n",
    "        X['log_distance'] = np.log1p(X['distance_to_region_center'])\n",
    "        X['log_distance_categoric'] = pd.qcut(X['log_distance'], q=4, \n",
    "                                        labels=['Muy corta', 'Corta', 'Media', 'Larga'])\n",
    "        return X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = pd.read_csv('data1.csv')\n",
    "df_2 = pd.read_csv('data2.csv')\n",
    "df_target = pd.read_csv('objetivo.csv')\n",
    "\n",
    "# Une los DataFrames de entrenamiento y combina con el target\n",
    "df = pd.concat([df_1, df_2], axis=0)\n",
    "df_final = pd.merge(df, df_target, on=\"id\")\n",
    "\n",
    "# Separa la variable objetivo\n",
    "y = df_final['status_group']\n",
    "X = df_final.drop(columns=['status_group'])\n",
    "\n",
    "#  Elimina columnas que no vayas a usar\n",
    "columns_to_drop = ['recorded_by']\n",
    "X = X.drop(columns=columns_to_drop)\n",
    "\n",
    "numeric_features = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "categorical_features = X.select_dtypes(include=['object']).columns.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        # Transformación para variables numéricas: imputar y escalar\n",
    "        ('num', Pipeline(steps=[\n",
    "            ('imputer', SimpleImputer(strategy=\"median\")),\n",
    "            ('scaler', MinMaxScaler())\n",
    "        ]), numeric_features),\n",
    "        # Transformación para variables categóricas: imputar y aplicar OneHotEncoding\n",
    "        ('cat', Pipeline(steps=[\n",
    "            ('imputer', SimpleImputer(strategy=\"most_frequent\")),\n",
    "            ('onehot', OneHotEncoder(drop='first', handle_unknown='ignore'))\n",
    "        ]), categorical_features)\n",
    "    ],\n",
    "    remainder='drop'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline completo que encadena los pasos de feature engineering y el modelo\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('date_features', DateFeaturesTransformer()),\n",
    "    ('location_cluster', LocationClusterTransformer(n_clusters=10)),\n",
    "    ('region_distance', RegionDistanceTransformer()),\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', RandomForestClassifier(n_estimators=100, random_state=42))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/giovannicapote/Documents/Master/Data Science/Tarea/.venv/lib/python3.13/site-packages/sklearn/preprocessing/_encoders.py:246: UserWarning: Found unknown categories in columns [0, 1, 2, 3, 5, 8, 11] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.802300785634119\n",
      "Classification Report:\n",
      "                          precision    recall  f1-score   support\n",
      "\n",
      "             functional       0.80      0.89      0.84      9724\n",
      "functional needs repair       0.58      0.33      0.42      1293\n",
      "         non functional       0.83      0.77      0.80      6803\n",
      "\n",
      "               accuracy                           0.80     17820\n",
      "              macro avg       0.74      0.66      0.69     17820\n",
      "           weighted avg       0.80      0.80      0.80     17820\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/giovannicapote/Documents/Master/Data Science/Tarea/.venv/lib/python3.13/site-packages/sklearn/preprocessing/_encoders.py:246: UserWarning: Found unknown categories in columns [0, 1, 2, 3, 5, 8, 11] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Primeras 5 predicciones:\n",
      "      id    status_group\n",
      "0  50785      functional\n",
      "1  51630      functional\n",
      "2  17168      functional\n",
      "3  45559  non functional\n",
      "4  49871      functional\n"
     ]
    }
   ],
   "source": [
    "#X = X.drop(columns=[\"latitude\", \"longitude\",\"distance_to_region_center\",\"log_distance\"])\n",
    "#X = X.drop(columns=[\"date_recorded\", \"year_recorded\", \"month_recorded\", \"day_recorded\", \"waterpoint_age\",\"construction_year\"])\n",
    "# Dividir los datos de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Entrenar el pipeline\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Evaluar el modelo en el set de prueba\n",
    "y_pred = pipeline.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "# ======================\n",
    "# PREDICCIÓN SOBRE NUEVOS DATOS\n",
    "# ======================\n",
    "\n",
    "# Cargar y procesar el DataFrame de predicción\n",
    "prediciton_df = pd.read_csv('predicion.csv')\n",
    "\n",
    "prediciton_df = prediciton_df.drop(columns=columns_to_drop)\n",
    "\n",
    "# Aplica el pipeline entrenado (se aplicarán todos los pasos de transformación de forma consistente)\n",
    "predictions = pipeline.predict(prediciton_df)\n",
    "\n",
    "# Agrega las predicciones y guarda el resultado\n",
    "prediciton_df[\"status_group\"] = predictions\n",
    "final_result = prediciton_df[['id', 'status_group']]\n",
    "final_result.to_csv('resultados_predicciones.csv', index=False)\n",
    "print(\"\\nPrimeras 5 predicciones:\")\n",
    "print(prediciton_df[['id', 'status_group']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importancia de las 10 características principales:\n",
      "                                       feature  importance\n",
      "51857               cat__waterpoint_type_other    0.037508\n",
      "51862         cat__waterpoint_type_group_other    0.034219\n",
      "51784         cat__extraction_type_class_other    0.027774\n",
      "51761               cat__extraction_type_other    0.026434\n",
      "51827                     cat__quantity_enough    0.025959\n",
      "9                       num__construction_year    0.024529\n",
      "51831               cat__quantity_group_enough    0.021328\n",
      "1                              num__amount_tsh    0.019646\n",
      "3                               num__longitude    0.014721\n",
      "51775         cat__extraction_type_group_other    0.014658\n",
      "51810              cat__payment_type_never pay    0.013968\n",
      "2                              num__gps_height    0.013699\n",
      "51826               cat__quality_group_unknown    0.013679\n",
      "4                                num__latitude    0.012670\n",
      "51770       cat__extraction_type_group_gravity    0.011272\n",
      "51755             cat__extraction_type_gravity    0.010994\n",
      "51852  cat__waterpoint_type_communal standpipe    0.010850\n",
      "51782      cat__extraction_type_class_handpump    0.010610\n",
      "51794                      cat__management_vwc    0.010195\n",
      "8                              num__population    0.009843\n"
     ]
    }
   ],
   "source": [
    "# Extraer los nombres de las features del preprocesador\n",
    "feature_names = pipeline.named_steps['preprocessor'].get_feature_names_out()\n",
    "\n",
    "# Obtener las importancias del clasificador\n",
    "importances = pipeline.named_steps['classifier'].feature_importances_\n",
    "\n",
    "# Crear un DataFrame para visualizar la importancia de cada feature\n",
    "feat_importances = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'importance': importances\n",
    "}).sort_values(by='importance', ascending=False)\n",
    "\n",
    "print(\"Importancia de las 10 características principales:\")\n",
    "print(feat_importances.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Pipeline' object has no attribute 'corr'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[35]\u001b[39m\u001b[32m, line 23\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# Crear el objeto GridSearchCV\u001b[39;00m\n\u001b[32m     15\u001b[39m grid_search = GridSearchCV(\n\u001b[32m     16\u001b[39m     estimator=rf,\n\u001b[32m     17\u001b[39m     param_grid=param_grid,\n\u001b[32m   (...)\u001b[39m\u001b[32m     21\u001b[39m     verbose=\u001b[32m2\u001b[39m\n\u001b[32m     22\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m corr_matrix = \u001b[43mpipeline\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcorr\u001b[49m().abs()\n\u001b[32m     25\u001b[39m \u001b[38;5;66;03m# Extraer la parte superior del triángulo de la matriz\u001b[39;00m\n\u001b[32m     26\u001b[39m upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=\u001b[32m1\u001b[39m).astype(\u001b[38;5;28mbool\u001b[39m))\n",
      "\u001b[31mAttributeError\u001b[39m: 'Pipeline' object has no attribute 'corr'"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Definir la grilla de parámetros\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [10, 20, None],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'min_samples_leaf': [1, 2]\n",
    "}\n",
    "\n",
    "# Configurar el modelo base\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Crear el objeto GridSearchCV\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=rf,\n",
    "    param_grid=param_grid,\n",
    "    cv=3,  # 3-fold cross-validation\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,  # Usa todos los núcleos disponibles\n",
    "    verbose=2\n",
    ")\n",
    "corr_matrix = X.corr().abs()\n",
    "\n",
    "# Extraer la parte superior del triángulo de la matriz\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "\n",
    "# Identificar columnas con alta correlación (> 0.9)\n",
    "to_drop = [column for column in upper.columns if any(upper[column] > 0.9)]\n",
    "\n",
    "# Eliminar columnas correlacionadas\n",
    "X_reduced = X.drop(columns=to_drop)\n",
    "\n",
    "# Ejecutar la búsqueda\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Mostrar los mejores parámetros\n",
    "print(\"🔍 Mejores hiperparámetros encontrados:\")\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "# Evaluar el mejor modelo\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred_best = best_model.predict(X_test)\n",
    "\n",
    "# Resultados\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "\n",
    "print(f\"\\n✅ Accuracy con mejor modelo: {accuracy_score(y_test, y_pred_best):.2%}\\n\")\n",
    "print(\"📊 Reporte de clasificación:\\n\")\n",
    "print(classification_report(y_test, y_pred_best))\n",
    "print(\"🧩 Matriz de confusión:\\n\")\n",
    "print(confusion_matrix(y_test, y_pred_best))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
