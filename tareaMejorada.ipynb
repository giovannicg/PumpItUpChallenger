{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.cluster import KMeans\n",
    "from geopy.distance import geodesic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DateFeaturesTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Extrae caracter√≠sticas de fecha y calcula la edad del punto de agua.\n",
    "    Se encarga de transformar 'date_recorded' y calcular 'waterpoint_age' a partir de 'construction_year'.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.waterpoint_age_median = None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        X = X.copy()\n",
    "        X[\"date_recorded\"] = pd.to_datetime(X[\"date_recorded\"])\n",
    "        X[\"year_recorded\"] = X[\"date_recorded\"].dt.year\n",
    "        X[\"month_recorded\"] = X[\"date_recorded\"].dt.month\n",
    "        X[\"day_recorded\"] = X[\"date_recorded\"].dt.day\n",
    "        X[\"waterpoint_age\"] = X[\"year_recorded\"] - X[\"construction_year\"]\n",
    "        # Calcula la mediana para imputar cuando construction_year==0\n",
    "        self.waterpoint_age_median = X.loc[X[\"construction_year\"] != 0, \"waterpoint_age\"].median()\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        X[\"date_recorded\"] = pd.to_datetime(X[\"date_recorded\"])\n",
    "        X[\"year_recorded\"] = X[\"date_recorded\"].dt.year\n",
    "        X[\"month_recorded\"] = X[\"date_recorded\"].dt.month\n",
    "        X[\"day_recorded\"] = X[\"date_recorded\"].dt.day\n",
    "        X[\"waterpoint_age\"] = X[\"year_recorded\"] - X[\"construction_year\"]\n",
    "        X.loc[X[\"construction_year\"] == 0, \"waterpoint_age\"] = self.waterpoint_age_median\n",
    "        bins = [-np.inf, 0, 8, 26, 53]\n",
    "        labels = ['Negativo/Inconsistente', 'Muy Baja', 'Baja', 'Alta']\n",
    "        X['waterpoint_age_category'] = pd.cut(X['waterpoint_age'], bins=bins, labels=labels, include_lowest=True)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LocationClusterTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Crea un cluster de ubicaci√≥n usando las columnas 'longitude' y 'latitude'.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_clusters=10, random_state=42):\n",
    "        self.n_clusters = n_clusters\n",
    "        self.random_state = random_state\n",
    "        self.kmeans = None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.kmeans = KMeans(n_clusters=self.n_clusters, random_state=self.random_state, n_init=10)\n",
    "        self.kmeans.fit(X[[\"longitude\", \"latitude\"]])\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        X[\"location_cluster\"] = self.kmeans.predict(X[[\"longitude\", \"latitude\"]])\n",
    "        return X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegionDistanceTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Calcula la distancia entre la ubicaci√≥n del punto y el centro (mediana) de la regi√≥n.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.region_centers = {}\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # Calcula el centro (mediana) de cada regi√≥n\n",
    "        self.region_centers = X.groupby(\"region\")[[\"latitude\", \"longitude\"]].median().to_dict('index')\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        def calc_distance(row):\n",
    "            region = row[\"region\"]\n",
    "            if region in self.region_centers:\n",
    "                center = self.region_centers[region]\n",
    "                return geodesic((row[\"latitude\"], row[\"longitude\"]),\n",
    "                                (center[\"latitude\"], center[\"longitude\"])).km\n",
    "            else:\n",
    "                return np.nan\n",
    "        X[\"distance_to_region_center\"] = X.apply(calc_distance, axis=1)\n",
    "        X['log_distance'] = np.log1p(X['distance_to_region_center'])\n",
    "        X['log_distance_categoric'] = pd.qcut(X['log_distance'], q=4, \n",
    "                                        labels=['Muy corta', 'Corta', 'Media', 'Larga'])\n",
    "        return X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = pd.read_csv('data1.csv')\n",
    "df_2 = pd.read_csv('data2.csv')\n",
    "df_target = pd.read_csv('objetivo.csv')\n",
    "\n",
    "# Une los DataFrames de entrenamiento y combina con el target\n",
    "df = pd.concat([df_1, df_2], axis=0)\n",
    "df_final = pd.merge(df, df_target, on=\"id\")\n",
    "\n",
    "# Separa la variable objetivo\n",
    "y = df_final['status_group']\n",
    "X = df_final.drop(columns=['status_group'])\n",
    "\n",
    "#  Elimina columnas que no vayas a usar\n",
    "columns_to_drop = ['recorded_by']\n",
    "X = X.drop(columns=columns_to_drop)\n",
    "\n",
    "numeric_features = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "categorical_features = X.select_dtypes(include=['object']).columns.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        # Transformaci√≥n para variables num√©ricas: imputar y escalar\n",
    "        ('num', Pipeline(steps=[\n",
    "            ('imputer', SimpleImputer(strategy=\"median\")),\n",
    "            ('scaler', MinMaxScaler())\n",
    "        ]), numeric_features),\n",
    "        # Transformaci√≥n para variables categ√≥ricas: imputar y aplicar OneHotEncoding\n",
    "        ('cat', Pipeline(steps=[\n",
    "            ('imputer', SimpleImputer(strategy=\"most_frequent\")),\n",
    "            ('onehot', OneHotEncoder(drop='first', handle_unknown='ignore'))\n",
    "        ]), categorical_features)\n",
    "    ],\n",
    "    remainder='drop'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline completo que encadena los pasos de feature engineering y el modelo\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('date_features', DateFeaturesTransformer()),\n",
    "    ('location_cluster', LocationClusterTransformer(n_clusters=10)),\n",
    "    ('region_distance', RegionDistanceTransformer()),\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', RandomForestClassifier(n_estimators=100, random_state=42))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/giovannicapote/Documents/Master/Data Science/Tarea/.venv/lib/python3.13/site-packages/sklearn/preprocessing/_encoders.py:246: UserWarning: Found unknown categories in columns [0, 1, 2, 3, 5, 8, 11] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.802300785634119\n",
      "Classification Report:\n",
      "                          precision    recall  f1-score   support\n",
      "\n",
      "             functional       0.80      0.89      0.84      9724\n",
      "functional needs repair       0.58      0.33      0.42      1293\n",
      "         non functional       0.83      0.77      0.80      6803\n",
      "\n",
      "               accuracy                           0.80     17820\n",
      "              macro avg       0.74      0.66      0.69     17820\n",
      "           weighted avg       0.80      0.80      0.80     17820\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/giovannicapote/Documents/Master/Data Science/Tarea/.venv/lib/python3.13/site-packages/sklearn/preprocessing/_encoders.py:246: UserWarning: Found unknown categories in columns [0, 1, 2, 3, 5, 8, 11] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Primeras 5 predicciones:\n",
      "      id    status_group\n",
      "0  50785      functional\n",
      "1  51630      functional\n",
      "2  17168      functional\n",
      "3  45559  non functional\n",
      "4  49871      functional\n"
     ]
    }
   ],
   "source": [
    "#X = X.drop(columns=[\"latitude\", \"longitude\",\"distance_to_region_center\",\"log_distance\"])\n",
    "#X = X.drop(columns=[\"date_recorded\", \"year_recorded\", \"month_recorded\", \"day_recorded\", \"waterpoint_age\",\"construction_year\"])\n",
    "# Dividir los datos de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Entrenar el pipeline\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Evaluar el modelo en el set de prueba\n",
    "y_pred = pipeline.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "# ======================\n",
    "# PREDICCI√ìN SOBRE NUEVOS DATOS\n",
    "# ======================\n",
    "\n",
    "# Cargar y procesar el DataFrame de predicci√≥n\n",
    "prediciton_df = pd.read_csv('predicion.csv')\n",
    "\n",
    "prediciton_df = prediciton_df.drop(columns=columns_to_drop)\n",
    "\n",
    "# Aplica el pipeline entrenado (se aplicar√°n todos los pasos de transformaci√≥n de forma consistente)\n",
    "predictions = pipeline.predict(prediciton_df)\n",
    "\n",
    "# Agrega las predicciones y guarda el resultado\n",
    "prediciton_df[\"status_group\"] = predictions\n",
    "final_result = prediciton_df[['id', 'status_group']]\n",
    "final_result.to_csv('resultados_predicciones.csv', index=False)\n",
    "print(\"\\nPrimeras 5 predicciones:\")\n",
    "print(prediciton_df[['id', 'status_group']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importancia de las 10 caracter√≠sticas principales:\n",
      "                                       feature  importance\n",
      "51857               cat__waterpoint_type_other    0.037508\n",
      "51862         cat__waterpoint_type_group_other    0.034219\n",
      "51784         cat__extraction_type_class_other    0.027774\n",
      "51761               cat__extraction_type_other    0.026434\n",
      "51827                     cat__quantity_enough    0.025959\n",
      "9                       num__construction_year    0.024529\n",
      "51831               cat__quantity_group_enough    0.021328\n",
      "1                              num__amount_tsh    0.019646\n",
      "3                               num__longitude    0.014721\n",
      "51775         cat__extraction_type_group_other    0.014658\n",
      "51810              cat__payment_type_never pay    0.013968\n",
      "2                              num__gps_height    0.013699\n",
      "51826               cat__quality_group_unknown    0.013679\n",
      "4                                num__latitude    0.012670\n",
      "51770       cat__extraction_type_group_gravity    0.011272\n",
      "51755             cat__extraction_type_gravity    0.010994\n",
      "51852  cat__waterpoint_type_communal standpipe    0.010850\n",
      "51782      cat__extraction_type_class_handpump    0.010610\n",
      "51794                      cat__management_vwc    0.010195\n",
      "8                              num__population    0.009843\n"
     ]
    }
   ],
   "source": [
    "# Extraer los nombres de las features del preprocesador\n",
    "feature_names = pipeline.named_steps['preprocessor'].get_feature_names_out()\n",
    "\n",
    "# Obtener las importancias del clasificador\n",
    "importances = pipeline.named_steps['classifier'].feature_importances_\n",
    "\n",
    "# Crear un DataFrame para visualizar la importancia de cada feature\n",
    "feat_importances = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'importance': importances\n",
    "}).sort_values(by='importance', ascending=False)\n",
    "\n",
    "print(\"Importancia de las 10 caracter√≠sticas principales:\")\n",
    "print(feat_importances.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Pipeline' object has no attribute 'corr'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[35]\u001b[39m\u001b[32m, line 23\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# Crear el objeto GridSearchCV\u001b[39;00m\n\u001b[32m     15\u001b[39m grid_search = GridSearchCV(\n\u001b[32m     16\u001b[39m     estimator=rf,\n\u001b[32m     17\u001b[39m     param_grid=param_grid,\n\u001b[32m   (...)\u001b[39m\u001b[32m     21\u001b[39m     verbose=\u001b[32m2\u001b[39m\n\u001b[32m     22\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m corr_matrix = \u001b[43mpipeline\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcorr\u001b[49m().abs()\n\u001b[32m     25\u001b[39m \u001b[38;5;66;03m# Extraer la parte superior del tri√°ngulo de la matriz\u001b[39;00m\n\u001b[32m     26\u001b[39m upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=\u001b[32m1\u001b[39m).astype(\u001b[38;5;28mbool\u001b[39m))\n",
      "\u001b[31mAttributeError\u001b[39m: 'Pipeline' object has no attribute 'corr'"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Definir la grilla de par√°metros\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [10, 20, None],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'min_samples_leaf': [1, 2]\n",
    "}\n",
    "\n",
    "# Configurar el modelo base\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Crear el objeto GridSearchCV\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=rf,\n",
    "    param_grid=param_grid,\n",
    "    cv=3,  # 3-fold cross-validation\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,  # Usa todos los n√∫cleos disponibles\n",
    "    verbose=2\n",
    ")\n",
    "corr_matrix = X.corr().abs()\n",
    "\n",
    "# Extraer la parte superior del tri√°ngulo de la matriz\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "\n",
    "# Identificar columnas con alta correlaci√≥n (> 0.9)\n",
    "to_drop = [column for column in upper.columns if any(upper[column] > 0.9)]\n",
    "\n",
    "# Eliminar columnas correlacionadas\n",
    "X_reduced = X.drop(columns=to_drop)\n",
    "\n",
    "# Ejecutar la b√∫squeda\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Mostrar los mejores par√°metros\n",
    "print(\"üîç Mejores hiperpar√°metros encontrados:\")\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "# Evaluar el mejor modelo\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred_best = best_model.predict(X_test)\n",
    "\n",
    "# Resultados\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "\n",
    "print(f\"\\n‚úÖ Accuracy con mejor modelo: {accuracy_score(y_test, y_pred_best):.2%}\\n\")\n",
    "print(\"üìä Reporte de clasificaci√≥n:\\n\")\n",
    "print(classification_report(y_test, y_pred_best))\n",
    "print(\"üß© Matriz de confusi√≥n:\\n\")\n",
    "print(confusion_matrix(y_test, y_pred_best))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
