{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mensemble\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RandomForestClassifier\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Load the data\n",
    "df_1= pd.read_csv('data1.csv')\n",
    "df_2= pd.read_csv('data2.csv')\n",
    "predcit_df = pd.read_csv('predicion.csv')\n",
    "\n",
    "df_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mdf_2\u001b[49m.head()\n",
      "\u001b[31mNameError\u001b[39m: name 'df_2' is not defined"
     ]
    }
   ],
   "source": [
    "df_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unir los dos dataframes\n",
    "df = pd.concat([df_1, df_2], axis=0)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Faltan datos en las columnas\n",
    "#### funder,installer,wpt_name,subvillage,public_meeting,scheme_management,scheme_management,permit,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ver que varables son categoricas y cuales numericas\n",
    "print(f'Variables Categoricas{df.select_dtypes(include=['object']).columns}')\n",
    "print(f'Varibales numericas{df.select_dtypes(include=['float64',\n",
    "                          'int64']).columns}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#comprobar valores unicos en todas las columnas\n",
    "for col in df.columns:\n",
    "    print(f'{col}: {df[col].unique()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### revisando los valores unicos de todas las variables se pueden hacer transformaciones en las variables:\n",
    "\n",
    "#### waterpoint_type_group,waterpoint_type_group,source_class,source_type,source,quantity_group\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# revisar valores nulos\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Se hace la primera transformacion para calcualr la antiguedad del punto de agua y ver si es nuevo, viejo,antiguo o media y asi saber con mas presicion si por ejemplo llos puntos de agua contruidos en ciertos años tienen mas fallas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir a datetime\n",
    "df[\"date_recorded\"] = pd.to_datetime(df[\"date_recorded\"])\n",
    "df[\"year_recorded\"] = df[\"date_recorded\"].dt.year\n",
    "df[\"month_recorded\"] = df[\"date_recorded\"].dt.month\n",
    "df[\"day_recorded\"] = df[\"date_recorded\"].dt.day\n",
    "\n",
    "predcit_df[\"date_recorded\"] = pd.to_datetime(predcit_df[\"date_recorded\"])\n",
    "predcit_df[\"year_recorded\"] = predcit_df[\"date_recorded\"].dt.year\n",
    "predcit_df[\"month_recorded\"] = predcit_df[\"date_recorded\"].dt.month\n",
    "predcit_df[\"day_recorded\"] = predcit_df[\"date_recorded\"].dt.day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"waterpoint_age\"] = df[\"year_recorded\"] - df[\"construction_year\"]\n",
    "df.loc[df[\"construction_year\"] == 0, \"waterpoint_age\"] = df[\"waterpoint_age\"].median()\n",
    "\n",
    "predcit_df[\"waterpoint_age\"] = predcit_df[\"year_recorded\"] - predcit_df[\"construction_year\"]\n",
    "predcit_df.loc[predcit_df[\"construction_year\"] == 0, \"waterpoint_age\"] = predcit_df[\"waterpoint_age\"].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### La segunda transformacion que podemos realizar en agrupar puntos de agua por proximidad esto nos va a permitir saber si los puntos de agua mas alejados del centro tienen mas o menos problemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[[\"longitude\", \"latitude\", \"gps_height\"]].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Aplicar K-Means con 10 clusters (ajustable)\n",
    "kmeans = KMeans(n_clusters=10, random_state=42, n_init=10)\n",
    "df[\"location_cluster\"] = kmeans.fit_predict(df[[\"longitude\", \"latitude\"]])\n",
    "predcit_df[\"location_cluster\"] = kmeans.predict(predcit_df[[\"longitude\", \"latitude\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from geopy.distance import geodesic\n",
    "\n",
    "# Crear un diccionario con el centro de cada región\n",
    "region_centers = df.groupby(\"region\")[[\"latitude\", \"longitude\"]].median()\n",
    "region_centers2= predcit_df.groupby(\"region\")[[\"latitude\", \"longitude\"]].median()\n",
    "\n",
    "# Función para calcular la distancia geográfica\n",
    "def calcular_distancia(row):\n",
    "    centro = region_centers.loc[row[\"region\"]]\n",
    "    return geodesic((row[\"latitude\"], row[\"longitude\"]), (centro[\"latitude\"], centro[\"longitude\"])).km\n",
    "\n",
    "def calcular_distancia2(row):\n",
    "    centro = region_centers2.loc[row[\"region\"]]\n",
    "    return geodesic((row[\"latitude\"], row[\"longitude\"]), (centro[\"latitude\"], centro[\"longitude\"])).km\n",
    "\n",
    "# Aplicar la función\n",
    "df[\"distance_to_region_center\"] = df.apply(calcular_distancia, axis=1)\n",
    "predcit_df[\"distance_to_region_center\"] = predcit_df.apply(calcular_distancia2, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predcit_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### La altura (gps_height) puede ser una variable importante si hay patrones de falla en áreas más altas o bajas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "df[\"gps_height_scaled\"] = scaler.fit_transform(df[[\"gps_height\"]])\n",
    "predcit_df[\"gps_height_scaled\"] = scaler.transform(predcit_df[[\"gps_height\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tercera transformacion, convertimos las variables categoricas en numericas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 10  # Número mínimo de ocurrencias\n",
    "# Seleccionar columnas categóricas\n",
    "cat_cols = df.select_dtypes(include=['object']).columns\n",
    "cat_cols2 = predcit_df.select_dtypes(include=['object']).columns\n",
    "for col in cat_cols:\n",
    "    counts = df[col].value_counts()\n",
    "    mask = df[col].isin(counts[counts < threshold].index)\n",
    "    df[col] = np.where(mask, 'Otros', df[col])\n",
    "\n",
    "for col in cat_cols2:\n",
    "    counts = predcit_df[col].value_counts()\n",
    "    mask = predcit_df[col].isin(counts[counts < threshold].index)\n",
    "    predcit_df[col] = np.where(mask, 'Otros', predcit_df[col])\n",
    "    \n",
    "\n",
    "import category_encoders as ce\n",
    "\n",
    "# Aplicar Binary Encoding\n",
    "encoder = ce.BinaryEncoder(cols=cat_cols)\n",
    "encoder2 = ce.BinaryEncoder(cols=cat_cols2)\n",
    "df_encoded = encoder.fit_transform(df)\n",
    "df_encoded2 = encoder2.fit_transform(predcit_df)\n",
    "\n",
    "\n",
    "pd.options.display.max_columns = None\n",
    "print(df.head())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rellenar valores nulos con la moda de cada columna\n",
    "categorical_columns = [\"funder\", \"installer\", \"scheme_name\", \"scheme_management\",\n",
    "                       \"management\", \"payment\", \"water_quality\", \"quantity\", \"source\", \"waterpoint_type\"]\n",
    "\n",
    "for col in categorical_columns:\n",
    "    df[col].fillna(df[col].mode()[0], inplace=True)\n",
    "    \n",
    "for col in categorical_columns:\n",
    "    predcit_df[col].fillna(predcit_df[col].mode()[0], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para agrupar categorías poco frecuentes\n",
    "def reducir_categorias(df, col, threshold=0.01):\n",
    "    \"\"\"Agrupa en 'Otros' las categorías que representan menos del threshold (%) de los datos\"\"\"\n",
    "    counts = df[col].value_counts(normalize=True)\n",
    "    rare_categories = counts[counts < threshold].index\n",
    "    df[col] = df[col].replace(rare_categories, \"Otros\")\n",
    "    return df\n",
    "\n",
    "def reducir_categorias2(df, col, threshold=0.01):\n",
    "    \"\"\"Agrupa en 'Otros' las categorías que representan menos del threshold (%) de los datos\"\"\"\n",
    "    counts = df[col].value_counts(normalize=True)\n",
    "    rare_categories = counts[counts < threshold].index\n",
    "    df[col] = df[col].replace(rare_categories, \"Otros\")\n",
    "    return df\n",
    "\n",
    "# Aplicar a las variables con muchas categorías\n",
    "#for col in [\"funder\", \"installer\", \"scheme_name\"]:\n",
    "    #df = reducir_categorias(df, col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.get_dummies(df, columns=[\"water_quality\", \"quantity\", \"source\", \"waterpoint_type\"], drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Codificar las columnas categóricas\n",
    "for column in categorical_columns:\n",
    "    df[column] = label_encoder.fit_transform(df[column].astype(str))\n",
    "\n",
    "for column in categorical_columns:\n",
    "    predcit_df[column] = label_encoder.fit_transform(predcit_df[column].astype(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contar las categorías únicas por columna categórica\n",
    "category_counts = df[categorical_columns].nunique()\n",
    "category_counts2 = predcit_df[categorical_columns].nunique()\n",
    "\n",
    "# Imprimir las columnas con más de 20 categorías\n",
    "columns_to_drop = category_counts[category_counts > 20].index\n",
    "columns_to_drop2 = category_counts2[category_counts2 > 20].index\n",
    "print(f\"Eliminando las siguientes columnas con más de 20 categorías: {columns_to_drop}\")\n",
    "df = df.drop(columns=columns_to_drop)\n",
    "predcit_df = predcit_df.drop(columns=columns_to_drop2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cuarta Trasformacion Manejo de outlayers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seleccionar columnas numéricas\n",
    "numerical_columns = df.select_dtypes(include=[\"int64\", \"float64\"]).columns\n",
    "numerical_columns2 = predcit_df.select_dtypes(include=[\"int64\", \"float64\"]).columns\n",
    "\n",
    "# Función para identificar outliers\n",
    "def detectar_outliers(df, column):\n",
    "    Q1 = df[column].quantile(0.25)  # Percentil 25\n",
    "    Q3 = df[column].quantile(0.75)  # Percentil 75\n",
    "    IQR = Q3 - Q1  # Rango intercuartil\n",
    "    lower_bound = Q1 - 1.5 * IQR  # Límite inferior\n",
    "    upper_bound = Q3 + 1.5 * IQR  # Límite superior\n",
    "    return df[(df[column] < lower_bound) | (df[column] > upper_bound)]\n",
    "\n",
    "\n",
    "# Detectar outliers en todas las columnas numéricas\n",
    "outliers_dict = {col: detectar_outliers(df, col) for col in numerical_columns}\n",
    "outliers_dict2 = {col: detectar_outliers(predcit_df, col) for col in numerical_columns2}\n",
    "\n",
    "# Contar cuántos outliers hay por variable\n",
    "outliers_count = {col: len(outliers_dict[col]) for col in numerical_columns}\n",
    "print(\"Cantidad de outliers por variable:\", outliers_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_target = pd.read_csv('objetivo.csv')\n",
    "\n",
    "df_final = pd.merge(df, df_target, on=\"id\")\n",
    "df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ver las variables q no sean numericas\n",
    "print(f'Variables Categoricas{df_final.select_dtypes(include=[\"object\"]).columns}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = df_final.drop(columns=['recorded_by','date_recorded','subvillage'])\n",
    "predcit_df = predcit_df.drop(columns=['recorded_by','date_recorded','subvillage'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Codificar la variable objetivo 'status_group'\n",
    "le = LabelEncoder()\n",
    "df_final['status_group'] = le.fit_transform(df_final['status_group'])\n",
    "\n",
    "# Realizar One-Hot Encoding para las variables con muchas categorías\n",
    "df_final = pd.get_dummies(df_final, columns=['wpt_name', 'basin','region', 'lga', 'ward', \n",
    "                                              'extraction_type', 'extraction_type_group', 'extraction_type_class', \n",
    "                                              'management_group', 'payment_type', 'quality_group', 'quantity_group', \n",
    "                                              'source_type', 'source_class', 'waterpoint_type_group'], drop_first=True)\n",
    "\n",
    "predcit_df = pd.get_dummies(predcit_df, columns=['wpt_name', 'basin', 'region', 'lga', 'ward', \n",
    "                                              'extraction_type', 'extraction_type_group', 'extraction_type_class', \n",
    "                                              'management_group', 'payment_type', 'quality_group', 'quantity_group', \n",
    "                                              'source_type', 'source_class', 'waterpoint_type_group'], drop_first=True)\n",
    "\n",
    "# Realizar Label Encoding para las variables binarias\n",
    "le = LabelEncoder()\n",
    "df_final['public_meeting'] = le.fit_transform(df_final['public_meeting'])\n",
    "df_final['permit'] = le.fit_transform(df_final['permit'])\n",
    "\n",
    "predcit_df['public_meeting'] = le.fit_transform(predcit_df['public_meeting'])\n",
    "predcit_df['permit'] = le.fit_transform(predcit_df['permit'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predcit_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separar las variables independientes (X) y la variable dependiente (y)\n",
    "X = df_final.drop(columns=[\"status_group\"])  # Reemplaza \"target_column\" con el nombre de tu columna objetivo\n",
    "y = df_final[\"status_group\"]  # Variable objetivo\n",
    "\n",
    "# Dividir en conjunto de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Crear el modelo Random Forest\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Entrenar el modelo\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Realizar predicciones\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "\n",
    "# Evaluar el modelo\n",
    "print(\"Random Forest Accuracy:\", accuracy_score(y_test, y_pred_rf))\n",
    "print(\"Random Forest Classification Report:\\n\", classification_report(y_test, y_pred_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = predcit_df.copy()\n",
    "predictions = rf.predict(X_new)\n",
    "predcit_df[\"prediccion\"]= predictions\n",
    "predcit_df.to_csv('resultados_predicciones.csv', index=False)\n",
    "print(\"\\nPrimeras 5 predicciones:\")\n",
    "print(predcit_df[['id', 'prediccion']].head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
